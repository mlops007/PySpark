{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ef54c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark= SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port','0'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/itv009033/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a36a8",
   "metadata": {},
   "source": [
    "# 1. Write PySpark code to create a new dataframe with the data given below having 2 columns (‘season’) and (‘windspeed’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d42af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Spring\", 12.3),\n",
    "(\"Summer\", 10.5),\n",
    "(\"Autumn\", 8.2),\n",
    "(\"Winter\", 15.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "977dbbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spring', 12.3), ('Summer', 10.5), ('Autumn', 8.2), ('Winter', 15.1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0e8dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_schema = 'season string , windspeed float'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8165d1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|season|windspeed|\n",
      "+------+---------+\n",
      "|Spring|     12.3|\n",
      "|Summer|     10.5|\n",
      "|Autumn|      8.2|\n",
      "|Winter|     15.1|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_seasons = spark.createDataFrame(data, seasons_schema)\n",
    "df_seasons.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac7fd9",
   "metadata": {},
   "source": [
    "# 2. Consider the library management dataset located at the following path\n",
    "(/public/trendytech/datasets/library_data.json). Using PySpark, load the\n",
    "data into a Dataframe and enforce schema using StructType.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37ccdc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_df = spark.read\\\n",
    "              .json(\"/public/trendytech/datasets/library_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78f141f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-----------+--------------------+\n",
      "|               books|     library_name|   location|             members|\n",
      "+--------------------+-----------------+-----------+--------------------+\n",
      "|[{F. Scott Fitzge...|  Central Library|City Center|[{28, [B001], M00...|\n",
      "|[{George Orwell, ...|Community Library|     Suburb|[{42, [B003, B004...|\n",
      "+--------------------+-----------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lib_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342e770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb6ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField(\"library_name\", StringType()),\n",
    "StructField(\"location\", StringType()),\n",
    "StructField(\"books\", ArrayType(\n",
    "StructType([\n",
    "StructField(\"book_id\", StringType()),\n",
    "StructField(\"book_name\", StringType()),\n",
    "StructField(\"author\", StringType()),\n",
    "StructField(\"copies_available\", IntegerType())\n",
    "])\n",
    ")),\n",
    "StructField(\"members\", ArrayType(\n",
    "StructType([\n",
    "StructField(\"member_id\", StringType()),\n",
    "StructField(\"member_name\", StringType()),\n",
    "StructField(\"age\", IntegerType()),\n",
    "StructField(\"books_borrowed\", ArrayType(StringType()))\n",
    "])\n",
    "))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f2d747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "library_df = spark.read.schema(schema).json(\"/public/trendytech/datasets/library_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45bd30ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+--------------------+--------------------+\n",
      "|     library_name|   location|               books|             members|\n",
      "+-----------------+-----------+--------------------+--------------------+\n",
      "|  Central Library|City Center|[{B001, The Great...|[{M001, John Smit...|\n",
      "|Community Library|     Suburb|[{B003, 1984, Geo...|[{M003, Michael B...|\n",
      "+-----------------+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f3514",
   "metadata": {},
   "source": [
    "# 3. Given the dataset (/public/trendytech/datasets/train.csv), create a Dataframe using PySpark and perform the following operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5baaaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|train_number|train_name|seats_available|passenger_name|age|ticket_number|seat_number|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|         123|   Express|            100|          John| 25|         T123|         A1|\n",
      "|         123|   Express|            100|          Emma| 30|         T124|         B2|\n",
      "|         456| Superfast|            150|       Michael| 35|         T125|         C3|\n",
      "|         456| Superfast|            150|        Sophia| 40|         T126|         D4|\n",
      "|         789|     Local|             50|       William| 28|         T127|         E5|\n",
      "|         789|     Local|             50|        Sophia| 32|         T128|         F6|\n",
      "|         789|     Local|             50|        Oliver| 45|         T129|         G7|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferSchema\",\"true\").load(\"/public/trendytech/datasets/train.csv\")\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8297789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- train_number: integer (nullable = true)\n",
      " |-- train_name: string (nullable = true)\n",
      " |-- seats_available: integer (nullable = true)\n",
      " |-- passenger_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- ticket_number: string (nullable = true)\n",
      " |-- seat_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aca454",
   "metadata": {},
   "source": [
    "**a) Drop the columns passenger_name and age from the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30376b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = train_df.drop(\"passenger_name\", \"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b55d3f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+-------------+-----------+\n",
      "|train_number|train_name|seats_available|ticket_number|seat_number|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "|         123|   Express|            100|         T123|         A1|\n",
      "|         123|   Express|            100|         T124|         B2|\n",
      "|         456| Superfast|            150|         T125|         C3|\n",
      "|         456| Superfast|            150|         T126|         D4|\n",
      "|         789|     Local|             50|         T127|         E5|\n",
      "|         789|     Local|             50|         T128|         F6|\n",
      "|         789|     Local|             50|         T129|         G7|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d14559a",
   "metadata": {},
   "source": [
    "**b) Count the number of rows after removing duplicates of columns\n",
    "train_number and ticket_number.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4538925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+-------------+-----------+\n",
      "|train_number|train_name|seats_available|ticket_number|seat_number|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "|         789|     Local|             50|         T128|         F6|\n",
      "|         123|   Express|            100|         T124|         B2|\n",
      "|         123|   Express|            100|         T123|         A1|\n",
      "|         456| Superfast|            150|         T126|         D4|\n",
      "|         456| Superfast|            150|         T125|         C3|\n",
      "|         789|     Local|             50|         T127|         E5|\n",
      "|         789|     Local|             50|         T129|         G7|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.dropDuplicates([\"train_number\", \"ticket_number\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a096a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removal  7   After Removal  7\n"
     ]
    }
   ],
   "source": [
    "print('Before removal ', df1.count(), ' ', 'After Removal ', df2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4023f1a",
   "metadata": {},
   "source": [
    "**c) Count the number of unique train names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c6cfe05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>train_name</th></tr>\n",
       "<tr><td>Express</td></tr>\n",
       "<tr><td>Local</td></tr>\n",
       "<tr><td>Superfast</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+\n",
       "|train_name|\n",
       "+----------+\n",
       "|   Express|\n",
       "|     Local|\n",
       "| Superfast|\n",
       "+----------+"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select(\"train_name\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b686334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select(\"train_name\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4ff70f",
   "metadata": {},
   "source": [
    "# 4. You are working as a Data Engineer in a large retail company. The company has a dataset named \"sales_data.json\" that contains sales records from various stores. The dataset is stored in JSON format and may have some corrupt or malformed records due to occasional data quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d53fa",
   "metadata": {},
   "source": [
    "**Your task is to read the \"sales_data.json\" dataset\n",
    "(/public/trendytech/datasets/sales_data.json) using PySpark, utilizing\n",
    "different read modes to handle corrupt records. You need to create a\n",
    "Dataframe using pyspark and perform the following operations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a2c4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_schema = \"store_id int, product string, quantity int, revenue float\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029bb52",
   "metadata": {},
   "source": [
    "**1. Read the dataset using the \"permissive\" mode and count the number of records read.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cab46d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df0 = spark.read.schema(sales_schema).json(\"/public/trendytech/datasets/sales_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35895fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-------+\n",
      "|store_id|   product|quantity|revenue|\n",
      "+--------+----------+--------+-------+\n",
      "|       1|     Apple|      10|  100.0|\n",
      "|       2|    Banana|      15|   75.0|\n",
      "|       3|    Orange|      12|   90.0|\n",
      "|       4|     Mango|       8|  120.0|\n",
      "|       5|     Grape|      20|  150.0|\n",
      "|       6|Watermelon|       5|   50.0|\n",
      "|       7|Strawberry|      18|  108.0|\n",
      "|       8| Pineapple|      14|  140.0|\n",
      "|       9|    Cherry|       7|  105.0|\n",
      "|      10|      Pear|       9|   81.0|\n",
      "|      11| Blueberry|      11|   88.0|\n",
      "|      12|      Kiwi|      16|  128.0|\n",
      "|      13|     Peach|      13|   91.0|\n",
      "|      14|      Plum|       6|   54.0|\n",
      "|      15|     Lemon|      10|   70.0|\n",
      "|      16| Raspberry|      17|  136.0|\n",
      "|      17|   Coconut|       4|   80.0|\n",
      "|      18|   Avocado|      11|   99.0|\n",
      "|      19|Blackberry|       8|   64.0|\n",
      "|      20|         G|    null|    NaN|\n",
      "+--------+----------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef39bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.read.option(\"mode\", \"failfast\").schema(sales_schema).json(\"/public/trendytech/datasets/sales_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14f77488",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o150.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 4 times, most recent failure: Lost task 0.3 in stage 48.0 (TID 1022) (w01.itversity.com executor 1): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:70)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$9(JsonDataSource.scala:144)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse a value for data type int (current token: VALUE_STRING).\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:492)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$7(JsonDataSource.scala:140)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 23 more\nCaused by: java.lang.RuntimeException: Failed to parse a value for data type int (current token: VALUE_STRING).\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:375)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:355)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$4$1.applyOrElse(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$4$1.applyOrElse(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:343)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$4(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:397)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:96)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:95)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:343)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:95)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:467)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2622)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:462)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1393580c109c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msales_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o150.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 4 times, most recent failure: Lost task 0.3 in stage 48.0 (TID 1022) (w01.itversity.com executor 1): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:70)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$9(JsonDataSource.scala:144)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse a value for data type int (current token: VALUE_STRING).\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:492)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$7(JsonDataSource.scala:140)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 23 more\nCaused by: java.lang.RuntimeException: Failed to parse a value for data type int (current token: VALUE_STRING).\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:375)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:355)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$4$1.applyOrElse(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$4$1.applyOrElse(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:343)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$4(JacksonParser.scala:184)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:397)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:96)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:95)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:343)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:95)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:467)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2622)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:462)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "767f4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df1 = spark.read.option(\"mode\", \"dropmalformed\").schema(sales_schema).json(\"/public/trendytech/datasets/sales_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2772f8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-------+\n",
      "|store_id|   product|quantity|revenue|\n",
      "+--------+----------+--------+-------+\n",
      "|       1|     Apple|      10|  100.0|\n",
      "|       2|    Banana|      15|   75.0|\n",
      "|       3|    Orange|      12|   90.0|\n",
      "|       4|     Mango|       8|  120.0|\n",
      "|       5|     Grape|      20|  150.0|\n",
      "|       6|Watermelon|       5|   50.0|\n",
      "|       7|Strawberry|      18|  108.0|\n",
      "|       8| Pineapple|      14|  140.0|\n",
      "|       9|    Cherry|       7|  105.0|\n",
      "|      10|      Pear|       9|   81.0|\n",
      "|      11| Blueberry|      11|   88.0|\n",
      "|      12|      Kiwi|      16|  128.0|\n",
      "|      13|     Peach|      13|   91.0|\n",
      "|      14|      Plum|       6|   54.0|\n",
      "|      15|     Lemon|      10|   70.0|\n",
      "|      16| Raspberry|      17|  136.0|\n",
      "|      17|   Coconut|       4|   80.0|\n",
      "|      18|   Avocado|      11|   99.0|\n",
      "|      19|Blackberry|       8|   64.0|\n",
      "+--------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da1d5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_count = sales_df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4449c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_count = sales_df0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "942e5f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_count - drop_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e4860fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g02.itversity.com:43309\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0843df6e48>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69becb8",
   "metadata": {},
   "source": [
    "# 5. You have a hospital dataset with the following fields:\n",
    "``● patient_id (integer): Unique identifier for each patient.\n",
    "● admission_date (date): The date the patient was admitted to the\n",
    "hospital. (MM-dd-yyyy)\n",
    "● discharge_date (date): The date the patient was discharged from the\n",
    "hospital. (yyyy-MM-dd)\n",
    "● diagnosis (string): The diagnosed medical condition of the patient.\n",
    "● doctor_id (integer): The identifier of the doctor responsible for the\n",
    "patient's care.\n",
    "● total_cost (float): The total cost of the hospital stay for the patient.\n",
    "Using PySpark, load the data into a Dataframe and perform the following\n",
    "operations on the hospital dataset\n",
    "(/public/trendytech/datasets/hospital.csv):``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d246725",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_schema = \"patient_id int, admission_date date, discharge_date date, diagnosis string, doctor_id int,total_cost float\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3803c102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|       _c0|           _c1|           _c2|          _c3|      _c4|       _c5|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|\n",
      "|         1|    01-01-2022|    2022-01-10|    Pneumonia|      101|   5000.00|\n",
      "|         2|    02-05-2022|    2022-02-09| Appendicitis|      102|   7000.00|\n",
      "|         3|    03-12-2022|    2022-03-18|Fractured Arm|      103|   3500.00|\n",
      "|         4|    04-02-2022|    2022-04-08| Heart Attack|      104|  15000.00|\n",
      "|         5|    05-05-2022|    2022-05-07|    Influenza|      105|   2500.00|\n",
      "|         6|    06-10-2022|    2022-06-15| Appendicitis|      106|   8000.00|\n",
      "|         7|    07-20-2022|    2022-07-25|    Pneumonia|      107|   5500.00|\n",
      "|         8|    08-25-2022|    2022-09-01| Heart Attack|      108|  20000.00|\n",
      "|         9|    09-15-2022|    2022-09-22|Fractured Leg|      109|   6000.00|\n",
      "|        10|    10-05-2022|    2022-10-10| Appendicitis|      110|   7500.00|\n",
      "|        11|    11-02-2022|    2022-11-05|    Influenza|      111|   2800.00|\n",
      "|        12|    12-10-2022|    2022-12-18|    Pneumonia|      112|   6000.00|\n",
      "|        13|    01-02-2023|    2023-01-09| Heart Attack|      113|  18000.00|\n",
      "|        14|    02-14-2023|    2023-02-18| Appendicitis|      114|   7200.00|\n",
      "|        15|    03-20-2023|    2023-03-28|Fractured Arm|      115|   3800.00|\n",
      "|        16|    04-05-2023|    2023-04-11|    Influenza|      116|   2700.00|\n",
      "|        17|    05-08-2023|    2023-05-11| Heart Attack|      117|  16000.00|\n",
      "|        18|    06-15-2023|    2023-06-20|    Pneumonia|      118|   4800.00|\n",
      "|        19|    07-22-2023|    2023-07-27|Fractured Leg|      119|   6500.00|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"/public/trendytech/datasets/hospital.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "671bdf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df = \\\n",
    "spark.read\\\n",
    ".format('csv')\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".schema(hospital_schema)\\\n",
    ".option(\"dateFormat\",\"MM-dd-yyyy\") \\\n",
    ".load(\"/public/trendytech/datasets/hospital.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "034f9662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|      101|    5000.0|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|      102|    7000.0|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|      103|    3500.0|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|      104|   15000.0|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|      105|    2500.0|\n",
      "|         6|    2022-06-10|    2022-06-15| Appendicitis|      106|    8000.0|\n",
      "|         7|    2022-07-20|    2022-07-25|    Pneumonia|      107|    5500.0|\n",
      "|         8|    2022-08-25|    2022-09-01| Heart Attack|      108|   20000.0|\n",
      "|         9|    2022-09-15|    2022-09-22|Fractured Leg|      109|    6000.0|\n",
      "|        10|    2022-10-05|    2022-10-10| Appendicitis|      110|    7500.0|\n",
      "|        11|    2022-11-02|    2022-11-05|    Influenza|      111|    2800.0|\n",
      "|        12|    2022-12-10|    2022-12-18|    Pneumonia|      112|    6000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      113|   18000.0|\n",
      "|        14|    2023-02-14|    2023-02-18| Appendicitis|      114|    7200.0|\n",
      "|        15|    2023-03-20|    2023-03-28|Fractured Arm|      115|    3800.0|\n",
      "|        16|    2023-04-05|    2023-04-11|    Influenza|      116|    2700.0|\n",
      "|        17|    2023-05-08|    2023-05-11| Heart Attack|      117|   16000.0|\n",
      "|        18|    2023-06-15|    2023-06-20|    Pneumonia|      118|    4800.0|\n",
      "|        19|    2023-07-22|    2023-07-27|Fractured Leg|      119|    6500.0|\n",
      "|        20|    2023-08-10|    2023-08-16| Appendicitis|      120|    7800.0|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf7bd171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|total_cost|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|    5000.0|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|    7000.0|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|    3500.0|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|   15000.0|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|    2500.0|\n",
      "|         6|    2022-06-10|    2022-06-15| Appendicitis|    8000.0|\n",
      "|         7|    2022-07-20|    2022-07-25|    Pneumonia|    5500.0|\n",
      "|         8|    2022-08-25|    2022-09-01| Heart Attack|   20000.0|\n",
      "|         9|    2022-09-15|    2022-09-22|Fractured Leg|    6000.0|\n",
      "|        10|    2022-10-05|    2022-10-10| Appendicitis|    7500.0|\n",
      "|        11|    2022-11-02|    2022-11-05|    Influenza|    2800.0|\n",
      "|        12|    2022-12-10|    2022-12-18|    Pneumonia|    6000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|   18000.0|\n",
      "|        14|    2023-02-14|    2023-02-18| Appendicitis|    7200.0|\n",
      "|        15|    2023-03-20|    2023-03-28|Fractured Arm|    3800.0|\n",
      "|        16|    2023-04-05|    2023-04-11|    Influenza|    2700.0|\n",
      "|        17|    2023-05-08|    2023-05-11| Heart Attack|   16000.0|\n",
      "|        18|    2023-06-15|    2023-06-20|    Pneumonia|    4800.0|\n",
      "|        19|    2023-07-22|    2023-07-27|Fractured Leg|    6500.0|\n",
      "|        20|    2023-08-10|    2023-08-16| Appendicitis|    7800.0|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df01 = hospital_df.drop(\"doctor_id\")\n",
    "df01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c7e0962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|       7000.0|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|       3500.0|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|      15000.0|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|       2500.0|\n",
      "|         6|    2022-06-10|    2022-06-15| Appendicitis|       8000.0|\n",
      "|         7|    2022-07-20|    2022-07-25|    Pneumonia|       5500.0|\n",
      "|         8|    2022-08-25|    2022-09-01| Heart Attack|      20000.0|\n",
      "|         9|    2022-09-15|    2022-09-22|Fractured Leg|       6000.0|\n",
      "|        10|    2022-10-05|    2022-10-10| Appendicitis|       7500.0|\n",
      "|        11|    2022-11-02|    2022-11-05|    Influenza|       2800.0|\n",
      "|        12|    2022-12-10|    2022-12-18|    Pneumonia|       6000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      18000.0|\n",
      "|        14|    2023-02-14|    2023-02-18| Appendicitis|       7200.0|\n",
      "|        15|    2023-03-20|    2023-03-28|Fractured Arm|       3800.0|\n",
      "|        16|    2023-04-05|    2023-04-11|    Influenza|       2700.0|\n",
      "|        17|    2023-05-08|    2023-05-11| Heart Attack|      16000.0|\n",
      "|        18|    2023-06-15|    2023-06-20|    Pneumonia|       4800.0|\n",
      "|        19|    2023-07-22|    2023-07-27|Fractured Leg|       6500.0|\n",
      "|        20|    2023-08-10|    2023-08-16| Appendicitis|       7800.0|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df01 = df01.withColumnRenamed(\"total_cost\",\"hospital_bill\")\n",
    "df01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cabaee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df02 = df01.selectExpr('*', \"DATEDIFF(discharge_date,admission_date) as duration_of_stay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8160270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|               9|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|       7000.0|               4|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|       3500.0|               6|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|      15000.0|               6|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|       2500.0|               2|\n",
      "|         6|    2022-06-10|    2022-06-15| Appendicitis|       8000.0|               5|\n",
      "|         7|    2022-07-20|    2022-07-25|    Pneumonia|       5500.0|               5|\n",
      "|         8|    2022-08-25|    2022-09-01| Heart Attack|      20000.0|               7|\n",
      "|         9|    2022-09-15|    2022-09-22|Fractured Leg|       6000.0|               7|\n",
      "|        10|    2022-10-05|    2022-10-10| Appendicitis|       7500.0|               5|\n",
      "|        11|    2022-11-02|    2022-11-05|    Influenza|       2800.0|               3|\n",
      "|        12|    2022-12-10|    2022-12-18|    Pneumonia|       6000.0|               8|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      18000.0|               7|\n",
      "|        14|    2023-02-14|    2023-02-18| Appendicitis|       7200.0|               4|\n",
      "|        15|    2023-03-20|    2023-03-28|Fractured Arm|       3800.0|               8|\n",
      "|        16|    2023-04-05|    2023-04-11|    Influenza|       2700.0|               6|\n",
      "|        17|    2023-05-08|    2023-05-11| Heart Attack|      16000.0|               3|\n",
      "|        18|    2023-06-15|    2023-06-20|    Pneumonia|       4800.0|               5|\n",
      "|        19|    2023-07-22|    2023-07-27|Fractured Leg|       6500.0|               5|\n",
      "|        20|    2023-08-10|    2023-08-16| Appendicitis|       7800.0|               6|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df02.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "065e2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df03 = df02.selectExpr(\"*\", \"CASE WHEN diagnosis LIKE 'Heart Attack' THEN hospital_bill * 1.5 WHEN diagnosis like 'Appendicitis' THEN hospital_bill *1.2 \\\n",
    "ELSE hospital_bill END AS adjusted_total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b4815",
   "metadata": {},
   "source": [
    "**a) Drop the columns passenger_name and age from the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "802d3ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|adjusted_total_cost|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|               9|             5000.0|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|       7000.0|               4|             8400.0|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|       3500.0|               6|             3500.0|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|      15000.0|               6|            22500.0|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|       2500.0|               2|             2500.0|\n",
      "|         6|    2022-06-10|    2022-06-15| Appendicitis|       8000.0|               5|             9600.0|\n",
      "|         7|    2022-07-20|    2022-07-25|    Pneumonia|       5500.0|               5|             5500.0|\n",
      "|         8|    2022-08-25|    2022-09-01| Heart Attack|      20000.0|               7|            30000.0|\n",
      "|         9|    2022-09-15|    2022-09-22|Fractured Leg|       6000.0|               7|             6000.0|\n",
      "|        10|    2022-10-05|    2022-10-10| Appendicitis|       7500.0|               5|             9000.0|\n",
      "|        11|    2022-11-02|    2022-11-05|    Influenza|       2800.0|               3|             2800.0|\n",
      "|        12|    2022-12-10|    2022-12-18|    Pneumonia|       6000.0|               8|             6000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      18000.0|               7|            27000.0|\n",
      "|        14|    2023-02-14|    2023-02-18| Appendicitis|       7200.0|               4|             8640.0|\n",
      "|        15|    2023-03-20|    2023-03-28|Fractured Arm|       3800.0|               8|             3800.0|\n",
      "|        16|    2023-04-05|    2023-04-11|    Influenza|       2700.0|               6|             2700.0|\n",
      "|        17|    2023-05-08|    2023-05-11| Heart Attack|      16000.0|               3|            24000.0|\n",
      "|        18|    2023-06-15|    2023-06-20|    Pneumonia|       4800.0|               5|             4800.0|\n",
      "|        19|    2023-07-22|    2023-07-27|Fractured Leg|       6500.0|               5|             6500.0|\n",
      "|        20|    2023-08-10|    2023-08-16| Appendicitis|       7800.0|               6|             9360.0|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df03.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "992fdb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+-------------------+----------------+\n",
      "|patient_id|    diagnosis|hospital_bill|adjusted_total_cost|duration_of_stay|\n",
      "+----------+-------------+-------------+-------------------+----------------+\n",
      "|        22| Heart Attack|      21000.0|            31500.0|               7|\n",
      "|         8| Heart Attack|      20000.0|            30000.0|               7|\n",
      "|        13| Heart Attack|      18000.0|            27000.0|               7|\n",
      "|        17| Heart Attack|      16000.0|            24000.0|               3|\n",
      "|         4| Heart Attack|      15000.0|            22500.0|               6|\n",
      "|         6| Appendicitis|       8000.0|             9600.0|               5|\n",
      "|        20| Appendicitis|       7800.0|             9360.0|               6|\n",
      "|        10| Appendicitis|       7500.0|             9000.0|               5|\n",
      "|        14| Appendicitis|       7200.0|             8640.0|               4|\n",
      "|         2| Appendicitis|       7000.0|             8400.0|               4|\n",
      "|        19|Fractured Leg|       6500.0|             6500.0|               5|\n",
      "|         9|Fractured Leg|       6000.0|             6000.0|               7|\n",
      "|        12|    Pneumonia|       6000.0|             6000.0|               8|\n",
      "|         7|    Pneumonia|       5500.0|             5500.0|               5|\n",
      "|        23|    Pneumonia|       5200.0|             5200.0|               7|\n",
      "|         1|    Pneumonia|       5000.0|             5000.0|               9|\n",
      "|        18|    Pneumonia|       4800.0|             4800.0|               5|\n",
      "|        24|Fractured Arm|       4100.0|             4100.0|               6|\n",
      "|        15|Fractured Arm|       3800.0|             3800.0|               8|\n",
      "|         3|Fractured Arm|       3500.0|             3500.0|               6|\n",
      "|        25|    Influenza|       3200.0|             3200.0|               5|\n",
      "|        21|    Influenza|       2900.0|             2900.0|               4|\n",
      "|        11|    Influenza|       2800.0|             2800.0|               3|\n",
      "|        16|    Influenza|       2700.0|             2700.0|               6|\n",
      "|         5|    Influenza|       2500.0|             2500.0|               2|\n",
      "+----------+-------------+-------------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df03.select(\"patient_id\", \"diagnosis\", \"hospital_bill\", \"adjusted_total_cost\", \"duration_of_stay\")\\\n",
    ".orderBy(\"adjusted_total_cost\", ascending = False).show(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a2804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
